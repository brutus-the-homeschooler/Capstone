# -*- coding: utf-8 -*-
"""City of Dublin K-Means Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19ozCPY5Ntehc1M0tDWk8Xkew3-OXL_lw

### Import Step
"""

import requests
import sqlite3
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from scipy.spatial.distance import euclidean
from sklearn.metrics import silhouette_score

# URL of the .db file hosted on GitHub
db_url = "https://raw.githubusercontent.com/brutus-the-homeschooler/Capstone/main/Database/acsse_2022.db"

# Download the .db file
response = requests.get(db_url)

# Save the .db file locally
with open("acsse_2022.db", "wb") as db_file:
    db_file.write(response.content)

# Connect to the SQLite database
conn = sqlite3.connect('acsse_2022.db')

# Query the 'census_data' table
census_data_df = pd.read_sql_query("SELECT * FROM census_data", conn)

# Query the 'place_dictionary' table
place_dict_df = pd.read_sql_query("SELECT * FROM place_dictionary", conn)

# Query the 'state_dictionary' table
#state_dict_df = pd.read_sql_query("SELECT * FROM state_dictionary", conn)

# Query the 'variable_dictionary' table
#variable_dict_df = pd.read_sql_query("SELECT * FROM variable_dictionary", conn)

# Close the connection
conn.close()

"""### Filter Variables

Based upon prior analysis, these 19 variables have been selected as reliable and interesting to study for the basis of this analysis.
"""

# Define filter variables for estimates (ending with 'E')
filter_variables = [
    "K200101_001E", "K200101_002E", "K200101_003E", "K200201_002E", "K200301_002E",
    "K200501_002E", "K200801_002E", "K200901_003E", "K201401_002E", "K201501_007E",
    "K201501_008E", "K201601_002E", "K201702_006E", "K202101_006E", "K202301_004E",
    "K202301_007E", "K202502_002E", "K202505_004E", "K202701_006E"
]

"""### Find reliable places

This code block takes the variables listed above and calculates the coefficient of variation based on the measurement value and the measurement of error, both of which are provided within the dataset.
"""

# Step 1: Find columns in census_data_df that match filter_variables for 'E' and their corresponding 'M'
estimate_cols = [col for col in census_data_df.columns if col in filter_variables]
moe_cols = [col[:-1] + 'M' for col in estimate_cols]

# Step 2: Calculate all CVs at once and store in new columns
for est_col, moe_col in zip(estimate_cols, moe_cols):
    if moe_col in census_data_df.columns:
        var_base = est_col[:-1]
        # Calculate CV for each row and add it as a separate column
        census_data_df[f'{var_base}_CV'] = (abs(census_data_df[moe_col]) / (1.645 * census_data_df[est_col])) * 100

# Step 3: Select CV columns and count how many have CV < 15 for each place-state pair
cv_columns = [col for col in census_data_df.columns if col.endswith('_CV')]
cv_data = census_data_df[['place', 'state'] + cv_columns].copy()

# Step 4: Group by 'place' and 'state' and count how many variables have a CV < 15 for each pair
cv_counts = (
    cv_data.melt(id_vars=['place', 'state'], value_vars=cv_columns, var_name='Variable', value_name='CV')
    .query('CV < 15')
    .groupby(['place', 'state'])
    .size()
    .reset_index(name='Count')
)

# Filter to rows where count of variables with CV < 15 is greater than 18
filtered_result_df = cv_counts[cv_counts['Count'] > 18]

# Step 5: Filter census_data_df based on filtered_result_df and select only the desired columns
filtered_census_data_df = census_data_df.merge(
    filtered_result_df[['place', 'state']].drop_duplicates(),
    on=['place', 'state'],
    how='inner'
)[['place', 'state'] + filter_variables]

"""### Make a backup just in case"""

data = filtered_census_data_df.copy()

"""### Scale the data"""

# Identify columns ending with 'E' for scaling
columns_to_scale = [col for col in data.columns if col.endswith('E')]

# Separate the identifier columns (place and state) and the columns to scale
data_identifiers = data[['place', 'state']]
data_to_scale = data[columns_to_scale]

# Scale the data
scaler = StandardScaler()
data_scaled_df = pd.DataFrame(scaler.fit_transform(data_to_scale), columns=columns_to_scale, index=data.index)

# Combine scaled columns with the identifiers (place and state)
data_final = pd.concat([data_identifiers, data_scaled_df], axis=1)

"""### Find optimal # for k"""

# Define range for number of clusters to test
K_range = range(1, 30)

# Calculate inertia for each k in the range
inertia = [KMeans(n_clusters=k, random_state=0).fit(data_scaled_df).inertia_ for k in K_range]

# Plot the Elbow Curve
plt.plot(K_range, inertia, marker='o')
plt.xlabel('Number of clusters (k)')
plt.ylabel('Inertia')
plt.title('Elbow Method for Optimal k')
plt.show()

"""We see the elbow starts between 8-12."""

# Define range for number of clusters to test
K_range = range(2, 30)

# Calculate silhouette scores for each k in the range
silhouette_scores = [
    silhouette_score(data_scaled_df, KMeans(n_clusters=k, random_state=0).fit_predict(data_scaled_df))
    for k in K_range
]

# Plot silhouette scores
plt.plot(K_range, silhouette_scores, marker='o')
plt.xlabel('Number of clusters (k)')
plt.ylabel('Silhouette Score')
plt.title('Silhouette Analysis for Optimal k')
plt.show()

"""The silhouette analysis shows that the silhouette score declines sharply between
ùëò
=
3 and
ùëò
=
6
, but it stabilizes somewhat around
ùëò
=
8
 to
ùëò
=
10
, supporting that
ùëò
=
10
could be a reasonable choice for clustering
"""

# Fit KMeans model with the chosen number of clusters
k_optimal = 10  # Replace with the chosen optimal number of clusters
kmeans = KMeans(n_clusters=k_optimal, random_state=0)
data_final['Cluster'] = kmeans.fit_predict(data_final)

# Find Dublin's cluster
Dublin_cluster = data_final[(data_final['state'] == '39') &
                                         (data_final['place'] == '22694')]['Cluster'].values[0]
print("Cluster for Dublin, Ohio:")
print(Dublin_cluster)

# Filter for rows where state is 39, which is Ohio
state_39_clusters = data_final[data_final['state'] == '39']

# Count the number of entries in each cluster for state 39
cluster_counts_state_39 = state_39_clusters['Cluster'].value_counts().sort_index()
print("\nCluster counts for Ohio:")
print(cluster_counts_state_39)

# Count the number of entries in each cluster overall
overall_cluster_counts = data_final['Cluster'].value_counts().sort_index()
print("\nOverall cluster counts:")
print(overall_cluster_counts)

"""We find that Dublin is in Cluster 0 along with 65 other places. Through the k-means clustering, we are able to bring down the possible sister cities to 65.

### Analyze Dublin's Cluster
"""

# Regions are based off of those at
# https://www2.census.gov/geo/pdfs/maps-data/maps/reference/us_regdiv.pdf

# Create a dictionary to map states to divisions

states_to_division = {

    'Connecticut':'New England','Maine':'New England','Massachusetts':'New England',
    'New Hampshire':'New England','Rhode Island':'New England','Vermont':'New England',

    'New Jersey': 'Middle Atlantic', 'Pennsylvania': 'Middle Atlantic','New York'	: 'Middle Atlantic',

    'Indiana': 'East North Central','Illinois': 'East North Central' ,'Michigan': 'East North Central',
    'Ohio': 'East North Central','Wisconsin': 'East North Central',

    'Iowa'	 :'West North Central','Nebraska'	:'West North Central','Kansas' 	:'West North Central',
    'North Dakota'	:'West North Central','Minnesota'	:'West North Central' ,'South Dakota'	:'West North Central',
    'Missouri':'West North Central',

    'Delaware' :'South Atlantic', 'District of Columbia'	:'South Atlantic',
    'Florida' :'South Atlantic','Georgia' :'South Atlantic','Maryland' :'South Atlantic',
    'North Carolina' :'South Atlantic','South Carolina' :'South Atlantic',
    'Virginia' :'South Atlantic','West Virginia' :'South Atlantic',

    'Alabama': 'East South Central','Kentucky': 'East South Central',
    'Mississippi': 'East South Central','Tennessee': 'East South Central',

    'Arkansas': 'West South Central', 'Louisiana': 'West South Central',
    'Oklahoma':'West South Central','Texas':'West South Central',

    'Arizona': 'Mountain', 'Colorado': 'Mountain','Idaho': 'Mountain', 'New Mexico': 'Mountain',
    'Montana': 'Mountain', 'Utah': 'Mountain','Nevada': 'Mountain','Wyoming': 'Mountain',

    'Washington': 'Pacific', 'Oregon': 'Pacific', 'California': 'Pacific','Alaska': 'Pacific', 'Hawaii': 'Pacific'}

division_to_region = {
   'New England':'Northeast','Middle Atlantic':'Northeast',
   'East North Central': 'Midwest', 'West North Central': 'Midwest',
   'South Atlantic': 'South', 'East South Central': 'South',  'West South Central': 'South',
   'Mountain': 'West', 'Pacific': 'West'}

# Merge place information into data_final
filtered_census_data_df = data_final.merge(
    place_dict_df[['place', 'state', 'City Name', 'State_Name']],
    on=['place', 'state'],
    how='inner'
)

# Map states to divisions and divisions to regions
filtered_census_data_df['Division'] = filtered_census_data_df['State_Name'].map(states_to_division)
filtered_census_data_df['Region'] = filtered_census_data_df['Division'].map(division_to_region)

# Identify the target cluster for state = 39 and place = 22694
target_cluster = filtered_census_data_df[
    (filtered_census_data_df['state'] == '39') &
    (filtered_census_data_df['place'] == '22694')
]['Cluster'].values[0]

# Filter for rows with the same cluster value
cluster_subset = filtered_census_data_df[filtered_census_data_df['Cluster'] == target_cluster]

# Group by State_Name, Region, and Division, and count occurrences
grouped_counts = {
    group_by: cluster_subset.groupby(group_by).size().reset_index(name='Count')
    for group_by in ['State_Name', 'Region', 'Division']
}

# Access individual group count DataFrames
counts_by_state = grouped_counts['State_Name']
counts_by_region = grouped_counts['Region']
counts_by_division = grouped_counts['Division']

# Display the result
print("Counts by State:")
print(counts_by_state)

print("\nCounts by Division:")
print(counts_by_division)

print("\nCounts by Region:")
print(counts_by_region)

"""### Below is the same k-means clustering based on the survey data where the values are the percentage compared to the total population variable. For example .487 for Male means the population is 48.7% Male."""

# Import the CSV file into a DataFrame
census_clean_df = pd.read_csv('/content/CensusData_Clean.csv')

# Preview the first few rows of the DataFrame
#print(census_clean_df.head())

# Identify columns ending with 'E' for scaling
columns_to_scale_2 = [col for col in census_clean_df.columns if col not in ['place', 'state']]

# Separate the identifier columns ('place' and 'state') and the columns to scale
data_identifiers_2 = census_clean_df[['place', 'state']]
data_to_scale_2 = census_clean_df[columns_to_scale_2]

# Scale the data
scaler = StandardScaler()
data_scaled_df_2 = pd.DataFrame(scaler.fit_transform(data_to_scale_2), columns=columns_to_scale_2, index=census_clean_df.index)

# Combine scaled columns with the identifiers (place and state)
data_final_2 = pd.concat([data_identifiers_2, data_scaled_df_2], axis=1)

# Define range for number of clusters to test
K_range = range(3,17)

# Calculate inertia for each k in the range
inertia = [KMeans(n_clusters=k, random_state=0).fit(data_scaled_df_2).inertia_ for k in K_range]

# Plot the Elbow Curve
plt.plot(K_range, inertia, marker='o')
plt.xlabel('Number of clusters (k)')
plt.ylabel('Inertia')
plt.title('Elbow Method for Optimal k')
plt.show()

# Define range for number of clusters to test
K_range = range(2, 15)

# Calculate silhouette scores for each k in the range
silhouette_scores = [
    silhouette_score(data_scaled_df_2, KMeans(n_clusters=k, random_state=0).fit_predict(data_scaled_df_2))
    for k in K_range
]

# Plot silhouette scores
plt.plot(K_range, silhouette_scores, marker='o')
plt.xlabel('Number of clusters (k)')
plt.ylabel('Silhouette Score')
plt.title('Silhouette Analysis for Optimal k')
plt.show()

# Drop non-numeric columns
data_numeric = data_final_2.drop(columns=['place', 'state'])

np.random.seed(142)
k_optimal =6  # Replace with the chosen optimal number of clusters
kmeans = KMeans(n_clusters=k_optimal, random_state=142)

# Fit and predict clusters
clusters = kmeans.fit_predict(data_numeric)

# Add the cluster assignments back to the numeric DataFrame
data_numeric['Cluster'] = clusters

# Add 'place' and 'state' back to the DataFrame
data_final_2 = pd.concat([data_final_2[['place', 'state']], data_numeric], axis=1)

# View the final DataFrame with clusters
print(data_final_2.head())

# Find Dublin's cluster
Dublin_cluster = data_final_2[(data_final_2['state'] == 'Ohio') &
                                         (data_final_2['place'] == 'Dublin city')]['Cluster'].values[0]
print("Cluster for Dublin, Ohio:")
print(Dublin_cluster)

# Filter for rows where state is 39, which is Ohio
state_39_clusters = data_final_2[data_final_2['state'] == 'Ohio']

# Count the number of entries in each cluster for state 39
cluster_counts_state_39 = state_39_clusters['Cluster'].value_counts().sort_index()
print("\nCluster counts for Ohio:")
print(cluster_counts_state_39)

# Count the number of entries in each cluster overall
overall_cluster_counts = data_final_2['Cluster'].value_counts().sort_index()
print("\nOverall cluster counts:")
print(overall_cluster_counts)

# Map Division and Region
data_final_2['Division'] = data_final_2['state'].map(states_to_division)
data_final_2['Region'] = data_final_2['Division'].map(division_to_region)

# Find Dublin, Ohio cluster
target_cluster = data_final_2[
    (data_final_2['state'] == 'Ohio') &
    (data_final_2['place'] == 'Dublin city')
]['Cluster'].values[0]

# Filter for rows with the same cluster value
cluster_subset_2 = data_final_2[data_final_2['Cluster'] == target_cluster]

# Group by State_Name, Region, and Division, and count occurrences
grouped_counts_2 = {
    group_by: cluster_subset_2.groupby(group_by).size().reset_index(name='Count')
    for group_by in ['state', 'Region', 'Division']
}

# Access individual group count DataFrames
counts_by_state_2 = grouped_counts_2['state']
counts_by_region_2 = grouped_counts_2['Region']
counts_by_division_2 = grouped_counts_2['Division']

# Display the result
print("Counts by State:")
print(counts_by_state_2)

print("\nCounts by Division:")
print(counts_by_division_2)

print("\nCounts by Region:")
print(counts_by_region_2)

# Only include numeric data
data_numeric = data_final_2.select_dtypes(include=['number'])

# Assuming you have already fitted the k-means model on your data
centroids = pd.DataFrame(kmeans.cluster_centers_, columns=data_numeric.columns)

# Heatmap
plt.figure(figsize=(16, 8))
sns.heatmap(centroids, annot=True, cmap='coolwarm', xticklabels=data_numeric.columns, yticklabels=[f"Cluster {i}" for i in range(len(centroids))])
plt.title("Heatmap of Cluster Centroids")
plt.xlabel("Variables")
plt.ylabel("Clusters")
plt.show()

"""
The heatmap of cluster centroids provides key insights into the characteristics of the variables within each cluster. Cluster 5 stands out with higher values for variables such as House_Built80to99, HealthIns_Covered, and Education_Bachelors, indicating that these features are particularly dominant in this cluster. In contrast, other clusters exhibit lower values for these variables, suggesting different population dynamics or socioeconomic trends. Cluster 5 also shows relatively high proportions of variables like Commute_DriveAlone and Employed, reinforcing its unique profile compared to the others. The visual gradient across the heatmap highlights the variability in each variable's influence on the clusters, with some variables such as PovertyLvl_2andOver and NonVeteran exhibiting consistently lower values across clusters. This variability allows for clear distinctions between clusters and helps in understanding the dominant traits that define Cluster 5 compared to the rest."""

# `kmeans.cluster_centers_` contains the centroids
# Add 'Cluster' column to the dataset if it's not already there
data_final_2['Cluster'] = kmeans.labels_

# Filter Cluster 5 data
cluster_3 = data_final_2[data_final_2['Cluster'] == 5]

# Get the centroid of Cluster 3
cluster_3_centroid = kmeans.cluster_centers_[5]  # Replace '3' with the cluster index for Cluster for Dublin
# Identify the numeric columns used in k-means clustering
columns_used_in_kmeans = data_numeric.columns  # Replace with the actual column names used in clustering

# Calculate Euclidean distance to the centroid for each point in Cluster 3
cluster_3['Distance_to_Centroid'] = cluster_3[columns_used_in_kmeans].apply(
    lambda row: euclidean(row, cluster_3_centroid),
    axis=1
)

# Check if Dublin City, OH is in Cluster 3
dublin_row = cluster_3[(cluster_3['place'] == 'Dublin city') & (cluster_3['state'] == 'Ohio')]

# Scatterplot for Cluster 5, colored by Region
plt.figure(figsize=(12, 6))

# Scatterplot with colors based on the 'Region' column
sns.scatterplot(
    x=cluster_3['Distance_to_Centroid'],
    y=[0] * len(cluster_3),  # Keep points horizontally aligned for clarity
    hue=cluster_3['Region'],  # Color by Region
    palette='tab10',          # Use a distinct color palette
    s=100                     # Size of points
)

# Highlight Dublin City, OH
plt.scatter(
    dublin_row['Distance_to_Centroid'],
    [0],
    color='red',
    marker='*',
    s=300,
    label='Dublin City, OH'
)

# Customize the plot
plt.title('Closeness of Places in Cluster 5 to Cluster Centroid (Colored by Region)')
plt.xlabel('Distance to Centroid')
plt.ylabel('Density (horizontal for visualization)')
plt.legend(title='Region', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.show()

"""Dublin City, OH (marked by the red star) is positioned relatively close to the centroid of Cluster 5, making it a strong representative of the cluster's overall characteristics. Its location on the lower end of the distance spectrum suggests that it is central to the cluster in terms of feature similarity. The points in the scatterplot are spread along the X-axis, which represents their Euclidean distances to the centroid; smaller distances indicate a closer alignment with the core features of Cluster 5. The Y-axis, meanwhile, serves purely for visual separation. Most places in Cluster 5 are grouped between distances of approximately 2 and 4, indicating a relatively compact cluster with a clear core, though there are a few outliers beyond a distance of 6. These outliers differ more significantly from the centroid and may not share as many characteristics with Dublin City or other central members of the cluster.

### Comparison between the two methods
"""

print("Counts by Region (Using raw values):")
print(counts_by_region)

print("\nCounts by Region (using percentages):")
print(counts_by_region_2)

"""Both methods highlight a strong reliance on the West and South regions, followed by the Midwest. Notably, Dublin, Ohio, is located in the Midwest region. This raises interesting questions about how potential sister cities could be concentrated in the South or West regions rather than within the same geographical area. The relatively high counts in the South and West suggest that cities in these regions may share similar socioeconomic or demographic characteristics with Dublin, Ohio, despite being geographically distant.

Paired with the earlier proximity-to-centroid visual analysis, this lends further support to examining the unique variables that position certain cities as closer matches to Dublin within the cluster. It also emphasizes the importance of expanding the analysis to include region-specific trends, such as population density, economic indicators, or cultural factors, that might influence cluster placement. This insight opens up opportunities to refine the analysis by incorporating additional regional variables or weighting certain features to better reflect Dublin's specific profile.

By combining the cluster proximity data with these regional distributions, the analysis gains a multidimensional perspective, helping narrow down potential sister cities while uncovering the broader relationships between regions and their shared attributes with Dublin, Ohio. This layered approach could significantly aid in making more informed decisions about which cities best align with Dublin's characteristics and priorities.
"""

print("Counts by Division (Using raw values):")
print(counts_by_division)
print("\nCounts by Division (using percentages):")
print(counts_by_division_2)

"""The division-level analysis closely mirrors the trends observed at the region level, with divisions like the Pacific, South Atlantic, and West South Central standing out in the upper bands. Interestingly, the East North Central division, which includes Ohio (and therefore Dublin), also ranks prominently, suggesting potential sister cities may exist both within Dublin's immediate geographic area and beyond.

The presence of divisions like Pacific and South Atlantic in the upper band indicates that cities in these divisions may share characteristics with Dublin despite being geographically distant. This underscores the value of exploring similarities beyond immediate proximity, such as economic indicators, demographic patterns, or cultural traits.

While the analysis reveals strong possibilities within the East North Central division‚Äîaligning with the expected regional proximity of Dublin‚Äîthis is balanced by notable representation from divisions in the South and West. This further validates the need for a holistic approach that combines geographic proximity with clustering variables to identify sister cities that align with Dublin‚Äôs unique attributes.
"""