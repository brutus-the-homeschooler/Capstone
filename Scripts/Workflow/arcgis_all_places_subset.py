# -*- coding: utf-8 -*-
"""arcgis_all_places_subset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dHtEq2-ub_XNDGpv1xo4zkONQNkCs-oi
"""

!pip install pygris geopandas

# Import required libraries
import requests
import sqlite3
import pandas as pd
import geopandas as gpd
from pygris import places

# URL of the .db file hosted on GitHub
db_url = "https://raw.githubusercontent.com/brutus-the-homeschooler/Capstone/main/Database/acsse_2022.db"

# Download the .db file
response = requests.get(db_url)

# Save the .db file locally
with open("acsse_2022.db", "wb") as db_file:
    db_file.write(response.content)

# Connect to the SQLite database
conn = sqlite3.connect('acsse_2022.db')

# Query the 'census_data' table
census_data_df = pd.read_sql_query("SELECT * FROM census_data", conn)

# Query the 'place_dictionary' table
place_dict_df = pd.read_sql_query("SELECT * FROM place_dictionary", conn)

# Query the 'state_dictionary' table
#state_dict_df = pd.read_sql_query("SELECT * FROM state_dictionary", conn)

# Query the 'variable_dictionary' table
#variable_dict_df = pd.read_sql_query("SELECT * FROM variable_dictionary", conn)

# Close the connection
conn.close()

# Filter variables picked in earlier stages of analysis
filter_variables = [
    "K200101_001E", "K200101_002E", "K200101_003E", "K200201_002E", "K200301_002E",
    "K200501_002E", "K200801_002E", "K200901_003E", "K201401_002E", "K201501_007E",
    "K201501_008E", "K201601_002E", "K201702_006E", "K202101_006E", "K202301_004E",
    "K202301_007E", "K202502_002E", "K202505_004E", "K202701_006E"
]

# Find columns in census_data_df that match filter_variables for 'E' and their corresponding 'M'
estimate_cols = [col for col in census_data_df.columns if col in filter_variables]
moe_cols = [col[:-1] + 'M' for col in estimate_cols]

# Calculate all CVs at once and store in new columns
for est_col, moe_col in zip(estimate_cols, moe_cols):
    if moe_col in census_data_df.columns:
        var_base = est_col[:-1]
        # Calculate CV for each row and add it as a separate column
        census_data_df[f'{var_base}_CV'] = (abs(census_data_df[moe_col]) / (1.645 * census_data_df[est_col])) * 100

# Select CV columns and count how many have CV < 15 for each place-state pair
cv_columns = [col for col in census_data_df.columns if col.endswith('_CV')]
cv_data = census_data_df[['place', 'state'] + cv_columns].copy()

# Group by 'place' and 'state' and count how many variables have a CV < 15 for each pair
cv_counts = (
    cv_data.melt(id_vars=['place', 'state'], value_vars=cv_columns, var_name='Variable', value_name='CV')
    .query('CV < 15')
    .groupby(['place', 'state'])
    .size()
    .reset_index(name='Count')
)

# Filter to rows where count of variables with CV < 15 is greater than 18
filtered_result_df = cv_counts[cv_counts['Count'] > 18]

# Filter census_data_df based on filtered_result_df and select only the desired columns
filtered_census_data_df = census_data_df.merge(
    filtered_result_df[['place', 'state']].drop_duplicates(),
    on=['place', 'state'],
    how='inner'
)[['place', 'state'] + filter_variables]

# make sure state/place are strings
place_dict_df['place'] = place_dict_df['place'].astype(str)
place_dict_df['state'] = place_dict_df['state'].astype(str)

filtered_census_data_df['place'] = filtered_census_data_df['place'].astype(str)
filtered_census_data_df['state'] = filtered_census_data_df['state'].astype(str)

# Merge dataframes
merged_df = pd.merge(place_dict_df, filtered_census_data_df, on=['place', 'state'], how='inner')

# Ensure FIPS codes are correctly padded in place_dict_df
merged_df['state'] = merged_df['state'].astype(str).str.zfill(2)
merged_df['place'] = merged_df['place'].astype(str).str.zfill(5)
merged_df['full_fips'] = merged_df['state'] + merged_df['place']

# Initialize an empty GeoDataFrame to store places
all_places_gdf = gpd.GeoDataFrame()

# Loop through unique state FIPS codes to fetch and filter places
for state_fips in merged_df['state'].unique():
    # Fetch place shapefiles for the specific state
    state_places_gdf = places(state=state_fips, cb=True)

    # Create a full FIPS code for the state shapefile
    state_places_gdf['full_fips'] = state_places_gdf['STATEFP'] + state_places_gdf['PLACEFP']

    # Filter the places based on matching FIPS codes from place_dict_df
    filtered_places_gdf = state_places_gdf[state_places_gdf['full_fips'].isin(merged_df['full_fips'])]

    # Append filtered places to the full GeoDataFrame
    all_places_gdf = pd.concat([all_places_gdf, filtered_places_gdf], ignore_index=True)

# Output as CSV (without geometry)
all_places_gdf.drop(columns='geometry').to_csv('2022_ACS_All_Places_Subset.csv', index=False)

# Output as GeoJSON (includes geometry)
all_places_gdf.to_file('2022_ACS_All_Places_Subset.geojson', driver='GeoJSON')

# Output as Shapefile
all_places_gdf.to_file('2022_ACS_All_Places_Subset.shp')